{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TomatoDataset/Otros/1, image ID: \n",
      "Processing TomatoDataset/Otros/2, image ID: image_5\n",
      "Processing TomatoDataset/Otros/3, image ID: image_10\n",
      "Processing TomatoDataset/Otros/4, image ID: image_16\n",
      "Processing TomatoDataset/Otros/5, image ID: image_19\n",
      "Processing TomatoDataset/Otros/SinVideo (anotado), image ID: image_25\n",
      "Processing TomatoDataset/rpi101_2024-06-06_14-40 (anotado)/1, image ID: image_37\n",
      "Processing TomatoDataset/rpi101_2024-06-06_14-40 (anotado)/2, image ID: image_42\n",
      "Processing TomatoDataset/rpi101_2024-06-06_14-40 (anotado)/3, image ID: image_46\n",
      "Processing TomatoDataset/rpi101_2024-06-06_14-40 (anotado)/4, image ID: image_51\n",
      "Processing TomatoDataset/rpi101_2024-07-04_12-31 (anotado)/1, image ID: image_54\n",
      "Processing TomatoDataset/rpi101_2024-07-04_12-31 (anotado)/2, image ID: image_59\n",
      "Processing TomatoDataset/rpi101_2024-07-04_12-31 (anotado)/3, image ID: image_63\n",
      "Processing TomatoDataset/rpi101_2024-07-04_12-31 (anotado)/4, image ID: image_68\n",
      "Processing TomatoDataset/rpi102_2024-07-04_12-31 (segmentado)/1, image ID: image_72\n",
      "Processing TomatoDataset/rpi102_2024-07-04_12-31 (segmentado)/2, image ID: image_79\n",
      "Processing TomatoDataset/rpi102_2024-07-04_12-31 (segmentado)/3, image ID: image_86\n",
      "Processing TomatoDataset/rpi102_2024-07-04_12-31 (segmentado)/4, image ID: image_90\n",
      "Processing TomatoDataset/rpi102_2025-02-07_10-13/1, image ID: image_95\n",
      "Processing TomatoDataset/rpi102_2025-02-07_10-13/3, image ID: image_103\n",
      "Processing TomatoDataset/rpi102_2025-02-07_10-13/4, image ID: image_111\n",
      "Processing TomatoDataset/rpi102_2025-02-17_12-02 (anotado)/1, image ID: image_117\n",
      "Processing TomatoDataset/rpi102_2025-02-17_12-02 (anotado)/2, image ID: image_122\n",
      "Processing TomatoDataset/rpi102_2025-02-17_12-02 (anotado)/3, image ID: image_127\n",
      "Processing TomatoDataset/rpi102_2025-02-17_12-02 (anotado)/4, image ID: image_132\n",
      "Processing TomatoDataset/rpi102_2025-04-14_16-47 (corregido)/1, image ID: image_137\n",
      "Processing TomatoDataset/rpi102_2025-04-14_16-47 (corregido)/2, image ID: image_146\n",
      "Processing TomatoDataset/rpi102_2025-04-14_16-47 (corregido)/3, image ID: image_156\n",
      "Processing TomatoDataset/rpi102_2025-04-14_16-47 (corregido)/4, image ID: image_167\n",
      "Processing TomatoDataset/rpi102_2025-04-25_14-14/1, image ID: image_177\n",
      "Processing TomatoDataset/rpi102_2025-04-25_14-14/2, image ID: image_185\n",
      "Processing TomatoDataset/rpi102_2025-04-25_14-14/3, image ID: image_193\n",
      "Processing TomatoDataset/rpi102_2025-04-25_14-14/4, image ID: image_201\n",
      "Processing TomatoDataset/rpi102_2025-07-17_16-55/1, image ID: image_209\n",
      "Processing TomatoDataset/rpi102_2025-07-17_16-55/2, image ID: image_219\n",
      "Processing TomatoDataset/rpi102_2025-07-17_16-55/3, image ID: image_227\n",
      "Processing TomatoDataset/rpi102_2025-07-17_16-55/4, image ID: image_235\n",
      "Processing TomatoDataset/rpi103_2024-07-12_12-33 (segmentado)/1, image ID: image_244\n",
      "Processing TomatoDataset/rpi103_2024-07-12_12-33 (segmentado)/2, image ID: image_248\n",
      "Processing TomatoDataset/rpi103_2024-07-12_12-33 (segmentado)/3, image ID: image_255\n",
      "Processing TomatoDataset/rpi103_2024-07-12_12-33 (segmentado)/4, image ID: image_260\n",
      "Processing TomatoDataset/rpi104_2025-07-17_16-55_Analizada/1_analizada, image ID: image_264\n",
      "Processing TomatoDataset/rpi104_2025-07-17_16-55_Analizada/2_analizada, image ID: image_272\n",
      "Processing TomatoDataset/rpi104_2025-07-17_16-55_Analizada/3_analizada, image ID: image_281\n",
      "Processing TomatoDataset/rpi104_2025-07-17_16-55_Analizada/4_analizada, image ID: image_290\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def natural_sort_key(s, _nsre=re.compile('([0-9]+)')):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(_nsre, s)]\n",
    "\n",
    "# Input directories\n",
    "path = \"TomatoDataset\"\n",
    "read_path = pathlib.Path(path).glob('*/*/*.png')\n",
    "image_paths = sorted([str(path) for path in read_path], key=natural_sort_key)\n",
    "\n",
    "# Output directories\n",
    "outpath = \"nnUNet_files/nnUNet_raw/Dataset555_TomateRoot/imagesTr\"\n",
    "labels_outpath = \"nnUNet_files/nnUNet_raw/Dataset555_TomateRoot/labelsTr\"\n",
    "\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "os.makedirs(labels_outpath, exist_ok=True)\n",
    "\n",
    "# Group images by folder\n",
    "folder_images = defaultdict(list)\n",
    "for path in image_paths:\n",
    "    # Get the folder path - using os.path for better compatibility\n",
    "    folder_path = os.path.dirname(path)\n",
    "    folder_images[folder_path].append(path)\n",
    "\n",
    "# Create a mapping dictionary that includes the folder info\n",
    "folder_image_map = defaultdict(list)\n",
    "\n",
    "# Counter for image naming\n",
    "counter = 1\n",
    "image_id = \"\"\n",
    "\n",
    "# Process all images grouped by folder\n",
    "for folder, images in folder_images.items():\n",
    "    folder_id = \"Manual_Annotation_\" + folder.replace(\"/\", \"_\").replace(\" \", \"_\")  # Create a safe folder ID\n",
    "\n",
    "    print(f'Processing {folder}, image ID: {image_id}')\n",
    "\n",
    "    # Process each image in the folder\n",
    "    for path in images:\n",
    "        # Create image ID\n",
    "        image_id = f'image_{counter}'\n",
    "        \n",
    "        # Copy the image to the output directory\n",
    "        shutil.copy(path, os.path.join(outpath, f'{image_id}_0000.png'))\n",
    "        \n",
    "        # Load the mask\n",
    "        mask_path = path.replace('.png', '.nii.gz')\n",
    "        mask = nib.load(mask_path).get_fdata().T\n",
    "        \n",
    "        if mask.shape[0] == 1:\n",
    "            mask = mask[0]\n",
    "        \n",
    "        image = cv2.imread(path, 0)\n",
    "        # Image shape and mask shape should be the same\n",
    "        if image.shape[0] != mask.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Save the mask\n",
    "        cv2.imwrite(os.path.join(labels_outpath, f'{image_id}.png'), mask.astype('uint8'))\n",
    "        \n",
    "        folder_image_map[folder_id].append(image_id)\n",
    "        \n",
    "        # Update the counter\n",
    "        counter += 1\n",
    "\n",
    "with open('nnUNet_files/nnUNet_raw/Dataset555_TomateRoot/folder_image_map.json', '+w') as f:\n",
    "    json.dump(folder_image_map, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 234 training images, 65 validation images\n",
      "Fold 1: 245 training images, 54 validation images\n",
      "Fold 2: 236 training images, 63 validation images\n",
      "Fold 3: 243 training images, 56 validation images\n",
      "Fold 4: 238 training images, 61 validation images\n",
      "Fold 5: 299 training images, 299 validation images\n",
      "\n",
      "Splits saved to: nnUNet_files/nnUNet_preprocessed/Dataset555_TomateRoot/splits_final.json\n",
      "Total folders: 45\n",
      "Folders per fold: [9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the folder_image_map.json file\n",
    "folder_map_path = 'nnUNet_files/nnUNet_raw/Dataset555_TomateRoot/folder_image_map.json'\n",
    "\n",
    "# Path to save the splits file\n",
    "splits_output_path = 'nnUNet_files/nnUNet_preprocessed/Dataset555_TomateRoot/splits_final.json'\n",
    "\n",
    "# Make directories if they don't exist\n",
    "os.makedirs(os.path.dirname(splits_output_path), exist_ok=True)\n",
    "\n",
    "# Load the folder-to-images mapping\n",
    "with open(folder_map_path, 'r') as f:\n",
    "    folder_image_map = json.load(f)\n",
    "\n",
    "# Get all folder IDs\n",
    "folder_ids = list(folder_image_map.keys())\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the folder IDs\n",
    "np.random.shuffle(folder_ids)\n",
    "\n",
    "# Split the folder IDs into 5 folds\n",
    "folder_folds = np.array_split(folder_ids, 5)\n",
    "\n",
    "# Create the 5-fold cross validation splits\n",
    "splits = []\n",
    "\n",
    "for i in range(5):\n",
    "    # Folders for validation in this fold\n",
    "    val_folders = folder_folds[i]\n",
    "    \n",
    "    # Initialize train and validation lists\n",
    "    train_images = []\n",
    "    val_images = []\n",
    "    \n",
    "    # Assign images to train or validation based on their folder\n",
    "    for folder_id in folder_ids:\n",
    "        if folder_id in val_folders:\n",
    "            # This folder goes to validation for this fold\n",
    "            val_images.extend(folder_image_map[folder_id])\n",
    "        else:\n",
    "            # This folder goes to training for this fold\n",
    "            train_images.extend(folder_image_map[folder_id])\n",
    "    \n",
    "    # Create the split dictionary\n",
    "    split = {\n",
    "        'train': train_images,\n",
    "        'val': val_images\n",
    "    }\n",
    "    \n",
    "    # Add to the splits list\n",
    "    splits.append(split)\n",
    "\n",
    "# Adds a fake folder 5 with all images in the training set for running inference later\n",
    "\n",
    "# Initialize train and validation lists\n",
    "train_images = []\n",
    "val_images = []\n",
    "\n",
    "# Assign images to train or validation based on their folder\n",
    "for folder_id in folder_ids:\n",
    "    val_images.extend(folder_image_map[folder_id])\n",
    "    train_images.extend(folder_image_map[folder_id])\n",
    "\n",
    "# Create the split dictionary\n",
    "split = {\n",
    "    'train': train_images,\n",
    "    'val': val_images\n",
    "}\n",
    "\n",
    "# Add to the splits list\n",
    "splits.append(split)\n",
    "\n",
    "# Print some statistics\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"Fold {i}: {len(split['train'])} training images, {len(split['val'])} validation images\")\n",
    "\n",
    "# Save the splits\n",
    "os.makedirs(os.path.dirname(splits_output_path), exist_ok=True)\n",
    "with open(splits_output_path, 'w') as f:\n",
    "    json.dump(splits, f, indent=4)\n",
    "\n",
    "print(f\"\\nSplits saved to: {splits_output_path}\")\n",
    "print(f\"Total folders: {len(folder_ids)}\")\n",
    "print(f\"Folders per fold: {[len(fold) for fold in folder_folds]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomato + Arabidopsis database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Input directories\n",
    "path2 = \"nnUNet_files/nnUNet_raw/Dataset789_ChronoRoot2/imagesTr\"\n",
    "path2_labels = \"nnUNet_files/nnUNet_raw/Dataset789_ChronoRoot2/labelsTr\"\n",
    "\n",
    "# Output directories\n",
    "outpath = \"nnUNet_files/nnUNet_raw/Dataset557_TArabidopsis/imagesTr\"\n",
    "labels_outpath = \"nnUNet_files/nnUNet_raw/Dataset557_TArabidopsis/labelsTr\"\n",
    "\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "os.makedirs(labels_outpath, exist_ok=True)\n",
    "\n",
    "# Combine the image and label paths into lists\n",
    "image_paths = [path2]\n",
    "\n",
    "# Process the image directories\n",
    "counter = 1\n",
    "for path in image_paths:\n",
    "    # Sort filenames before processing\n",
    "    while os.path.exists(os.path.join(path, f'image_{counter}_0000.png')):\n",
    "        filename = f'image_{counter}_0000.png'\n",
    "        shutil.copy(os.path.join(path, filename), os.path.join(outpath, f'image_{counter}_0000.png'))\n",
    "\n",
    "        label_path = os.path.join(path2_labels, filename.replace('_0000.png', '.png'))\n",
    "\n",
    "        label = cv2.imread(label_path, 0)\n",
    "        label[label == 6] = 5\n",
    "\n",
    "        cv2.imwrite(os.path.join(labels_outpath, f'image_{counter}.png'), label.astype('uint8'))\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Input directories\n",
    "path2 = \"nnUNet_files/nnUNet_raw/Dataset555_TomateRoot/imagesTr\"\n",
    "path2_labels = \"nnUNet_files/nnUNet_raw/Dataset555_TomateRoot/labelsTr\"\n",
    "\n",
    "# Output directories\n",
    "outpath = \"nnUNet_files/nnUNet_raw/Dataset557_TArabidopsis/imagesTr\"\n",
    "labels_outpath = \"nnUNet_files/nnUNet_raw/Dataset557_TArabidopsis/labelsTr\"\n",
    "\n",
    "# Combine the image and label paths into lists\n",
    "image_paths = [path2]\n",
    "\n",
    "old_counter = 1\n",
    "\n",
    "mapping = \"nnUNet_files/nnUNet_raw/Dataset555_TomateRoot/folder_image_map.json\"\n",
    "with open(mapping, 'r') as f:\n",
    "    folder_image_map = json.load(f)\n",
    "\n",
    "new_mapping = \"nnUNet_files/nnUNet_raw/Dataset789_ChronoRoot2/folder_image_map.json\"\n",
    "with open(new_mapping, 'r') as f:\n",
    "    new_folder_image_map = json.load(f)\n",
    "\n",
    "# Process the image directories\n",
    "for path in image_paths:\n",
    "    # Sort filenames before processing\n",
    "    while os.path.exists(os.path.join(path, f'image_{old_counter}_0000.png')):\n",
    "        filename = f'image_{old_counter}_0000.png'\n",
    "        shutil.copy(os.path.join(path, filename), os.path.join(outpath, f'image_{counter}_0000.png'))\n",
    "        \n",
    "        label_path = os.path.join(path2_labels, filename.replace('_0000.png', '.png'))\n",
    "        \n",
    "        shutil.copy(label_path, os.path.join(labels_outpath, f'image_{counter}.png'))\n",
    "                \n",
    "        # replace old_counter with in the mapping\n",
    "        for folder_id, images in folder_image_map.items():\n",
    "            if f\"image_{old_counter}\" in images:\n",
    "                if not folder_id in new_folder_image_map:\n",
    "                    new_folder_image_map[folder_id] = []\n",
    "                new_folder_image_map[folder_id].append(f'image_{counter}')\n",
    "        \n",
    "        counter += 1\n",
    "        old_counter += 1\n",
    "                \n",
    "# save it to tarabidopsis\n",
    "final_mapping_path = \"nnUNet_files/nnUNet_raw/Dataset557_TArabidopsis/folder_image_map.json\"\n",
    "with open(final_mapping_path, 'w') as f:\n",
    "    json.dump(new_folder_image_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147 Arabidopsis folders\n",
      "Found 45 Tomato folders\n",
      "\n",
      "Arabidopsis Dataset:\n",
      "Splits saved to: nnUNet_files/nnUNet_preprocessed/Dataset789_ChronoRoot2/splits_final.json\n",
      "Fold 0: 671 training images, 126 validation images\n",
      "Fold 1: 656 training images, 141 validation images\n",
      "Fold 2: 636 training images, 161 validation images\n",
      "Fold 3: 639 training images, 158 validation images\n",
      "Fold 4: 586 training images, 211 validation images\n",
      "Fold 5: 797 training images, 797 validation images\n",
      "Folders per fold: [30, 30, 29, 29, 29]\n",
      "\n",
      "Tomato Dataset:\n",
      "Splits saved to: nnUNet_files/nnUNet_preprocessed/Dataset555_TomateRoot/splits_final.json\n",
      "Fold 0: 247 training images, 52 validation images\n",
      "Fold 1: 244 training images, 55 validation images\n",
      "Fold 2: 238 training images, 61 validation images\n",
      "Fold 3: 230 training images, 69 validation images\n",
      "Fold 4: 237 training images, 62 validation images\n",
      "Fold 5: 299 training images, 299 validation images\n",
      "Folders per fold: [9, 9, 9, 9, 9]\n",
      "\n",
      "Creating Mixed Dataset Splits...\n",
      "\n",
      "Mixed Dataset:\n",
      "Splits saved to: nnUNet_files/nnUNet_preprocessed/Dataset557_TArabidopsis/splits_final.json\n",
      "Fold 0: 918 training images, 52 validation images\n",
      "Fold 1: 900 training images, 55 validation images\n",
      "Fold 2: 874 training images, 61 validation images\n",
      "Fold 3: 869 training images, 69 validation images\n",
      "Fold 4: 823 training images, 62 validation images\n",
      "Fold 5: 1096 training images, 299 validation images\n",
      "\n",
      "Total Arabidopsis folders: 147\n",
      "Total Tomato folders: 45\n",
      "All splits generation completed!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Path to the folder_image_map.json file\n",
    "folder_map_path = 'nnUNet_files/nnUNet_raw/Dataset557_TArabidopsis/folder_image_map.json'\n",
    "\n",
    "# Paths to save the splits files\n",
    "arabidopsis_splits_path = 'nnUNet_files/nnUNet_preprocessed/Dataset789_ChronoRoot2/splits_final.json'\n",
    "tomato_splits_path = 'nnUNet_files/nnUNet_preprocessed/Dataset555_TomateRoot/splits_final.json'\n",
    "mixed_splits_path = 'nnUNet_files/nnUNet_preprocessed/Dataset557_TArabidopsis/splits_final.json'\n",
    "\n",
    "# Load the folder-to-images mapping\n",
    "with open(folder_map_path, 'r') as f:\n",
    "    folder_image_map = json.load(f)\n",
    "\n",
    "# Separate folder IDs by dataset\n",
    "arabidopsis_folders = []\n",
    "tomato_folders = []\n",
    "\n",
    "for folder_id in folder_image_map.keys():\n",
    "    if 'Arabidopsis' in folder_id or 'arabidopsis' in folder_id:\n",
    "        arabidopsis_folders.append(folder_id)\n",
    "    elif 'Tomato' in folder_id or 'tomato' in folder_id:\n",
    "        tomato_folders.append(folder_id)\n",
    "    else:\n",
    "        # If folder doesn't contain either dataset name, you might want to handle this\n",
    "        print(f\"Warning: Folder '{folder_id}' doesn't contain 'Arabidopsis' or 'Tomato' in name\")\n",
    "\n",
    "print(f\"Found {len(arabidopsis_folders)} Arabidopsis folders\")\n",
    "print(f\"Found {len(tomato_folders)} Tomato folders\")\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_splits(folder_ids, folder_image_map):\n",
    "    \"\"\"Create 5-fold cross-validation splits for given folders\"\"\"\n",
    "    # Shuffle the folder IDs\n",
    "    folder_ids_shuffled = folder_ids.copy()\n",
    "    np.random.shuffle(folder_ids_shuffled)\n",
    "    \n",
    "    # Split the folder IDs into 5 folds\n",
    "    folder_folds = np.array_split(folder_ids_shuffled, 5)\n",
    "    \n",
    "    # Create the 5-fold cross validation splits\n",
    "    splits = []\n",
    "    for i in range(5):\n",
    "        # Folders for validation in this fold\n",
    "        val_folders = folder_folds[i]\n",
    "        \n",
    "        # Initialize train and validation lists\n",
    "        train_images = []\n",
    "        val_images = []\n",
    "        \n",
    "        # Assign images to train or validation based on their folder\n",
    "        for folder_id in folder_ids_shuffled:\n",
    "            if folder_id in val_folders:\n",
    "                # This folder goes to validation for this fold\n",
    "                val_images.extend(folder_image_map[folder_id])\n",
    "            else:\n",
    "                # This folder goes to training for this fold\n",
    "                train_images.extend(folder_image_map[folder_id])\n",
    "        \n",
    "        # Create the split dictionary\n",
    "        split = {\n",
    "            'train': train_images,\n",
    "            'val': val_images\n",
    "        }\n",
    "        \n",
    "        # Add to the splits list\n",
    "        splits.append(split)\n",
    "    \n",
    "    # Add a fake fold 6 with all images in both training and validation sets for inference\n",
    "    train_images = []\n",
    "    val_images = []\n",
    "    \n",
    "    for folder_id in folder_ids_shuffled:\n",
    "        val_images.extend(folder_image_map[folder_id])\n",
    "        train_images.extend(folder_image_map[folder_id])\n",
    "    \n",
    "    split = {\n",
    "        'train': train_images,\n",
    "        'val': val_images\n",
    "    }\n",
    "    splits.append(split)\n",
    "    \n",
    "    return splits, folder_folds\n",
    "\n",
    "def save_splits(splits, output_path, dataset_name, folder_folds):\n",
    "    \"\"\"Save splits to file and print statistics\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save the splits\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(splits, f, indent=4)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Dataset:\")\n",
    "    print(f\"Splits saved to: {output_path}\")\n",
    "    for i, split in enumerate(splits):\n",
    "        print(f\"Fold {i}: {len(split['train'])} training images, {len(split['val'])} validation images\")\n",
    "    print(f\"Folders per fold: {[len(fold) for fold in folder_folds]}\")\n",
    "\n",
    "import copy\n",
    "\n",
    "def save_tomato_splits(splits, output_path, dataset_name, folder_folds):\n",
    "    \"\"\"Save splits to file and print statistics\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # make a copy of splits\n",
    "    splits2 = copy.deepcopy(splits)\n",
    "    \n",
    "    # in each of the splits, image_NUMBER = image_NUMBER - 797\n",
    "    for split in splits2:\n",
    "        split['train'] = [img.replace(img, \"image_\" + str(int(img.split(\"_\")[1]) - 797)) for img in split['train']]\n",
    "        split['val'] = [img.replace(img, \"image_\" + str(int(img.split(\"_\")[1]) - 797)) for img in split['val']]\n",
    "        \n",
    "    # Save the splits\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(splits2, f, indent=4)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Dataset:\")\n",
    "    print(f\"Splits saved to: {output_path}\")\n",
    "    for i, split in enumerate(splits2):\n",
    "        print(f\"Fold {i}: {len(split['train'])} training images, {len(split['val'])} validation images\")\n",
    "    print(f\"Folders per fold: {[len(fold) for fold in folder_folds]}\")\n",
    "\n",
    "\n",
    "# Create splits for Arabidopsis dataset\n",
    "if arabidopsis_folders:\n",
    "    arabidopsis_splits, arabidopsis_folds = create_splits(arabidopsis_folders, folder_image_map)\n",
    "    save_splits(arabidopsis_splits, arabidopsis_splits_path, \"Arabidopsis\", arabidopsis_folds)\n",
    "\n",
    "# Create splits for Tomato dataset\n",
    "if tomato_folders:\n",
    "    tomato_splits, tomato_folds = create_splits(tomato_folders, folder_image_map)\n",
    "    save_tomato_splits(tomato_splits.copy(), tomato_splits_path, \"Tomato\", tomato_folds)\n",
    "\n",
    "# Create mixed dataset splits\n",
    "if arabidopsis_folders and tomato_folders:\n",
    "    print(f\"\\nCreating Mixed Dataset Splits...\")\n",
    "    \n",
    "    mixed_splits = []\n",
    "    \n",
    "    # Create 5 mixed folds by combining corresponding folds from both datasets\n",
    "    for i in range(5):\n",
    "        # Combine training images from both datasets for this fold\n",
    "        mixed_train = arabidopsis_splits[i]['train'] + tomato_splits[i]['train']\n",
    "        # Combine validation images from Tomato dataset only for this fold\n",
    "        mixed_val = tomato_splits[i]['val']\n",
    "        \n",
    "        mixed_split = {\n",
    "            'train': mixed_train,\n",
    "            'val': mixed_val\n",
    "        }\n",
    "        mixed_splits.append(mixed_split)\n",
    "    \n",
    "    # Add mixed inference fold (fold 6)\n",
    "    mixed_train_all = arabidopsis_splits[5]['train'] + tomato_splits[5]['train']\n",
    "    mixed_val_all = tomato_splits[5]['val']\n",
    "    \n",
    "    mixed_split_all = {\n",
    "        'train': mixed_train_all,\n",
    "        'val': mixed_val_all\n",
    "    }\n",
    "    mixed_splits.append(mixed_split_all)\n",
    "    \n",
    "    # Save mixed splits\n",
    "    os.makedirs(os.path.dirname(mixed_splits_path), exist_ok=True)\n",
    "    with open(mixed_splits_path, 'w') as f:\n",
    "        json.dump(mixed_splits, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nMixed Dataset:\")\n",
    "    print(f\"Splits saved to: {mixed_splits_path}\")\n",
    "    for i, split in enumerate(mixed_splits):\n",
    "        print(f\"Fold {i}: {len(split['train'])} training images, {len(split['val'])} validation images\")\n",
    "\n",
    "print(f\"\\nTotal Arabidopsis folders: {len(arabidopsis_folders)}\")\n",
    "print(f\"Total Tomato folders: {len(tomato_folders)}\")\n",
    "print(f\"All splits generation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnUNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
